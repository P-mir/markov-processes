}
return(c('state',state))
return(c('reward',reward)}
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=reward+1}
else{reward= 0}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
}
}
return(c('state',state))
return(c('reward',reward))}
TD0(10,0.1)
TD0(100,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=reward+1}
else{reward= 0}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
}
return(c('state',state))
return(c('reward',reward))
}
}
TD0(100,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=reward+1}
else{reward= 0}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
}
cat('state',state)
# return(c('state',state))
# return(c('reward',reward))
}
}
TD0(100,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=reward+1}
else{reward= 0}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
}
cat('fstate',state)
# return(c('state',state))
# return(c('reward',reward))
}
}
TD0(100,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=reward+1}
else{reward= 0}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
cat('state',state)
}
# return(c('state',state))
# return(c('reward',reward))
}
}
TD0(100,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=reward+1}
else{reward= 0}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
cat('state',state,"reward: ",reward,"\n")
}
# return(c('state',state))
# return(c('reward',reward))
}
}
TD0(100,0.1)
TD0(1000,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=0}
else{reward= reward+1}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
cat('state',state,"reward: ",reward,"\n")
}
cat("\n\n")
# return(c('state',state))
# return(c('reward',reward))
}
}
TD0(1000,0.1)
TD0=function(nEpisodes,alpha){
#set alpha to 0.1 for example
#discount factor, nothing here
gamma = 1
# the number of non terminal states plus two terminal states:
nStates=5+2
#initialize the nontemrinal states, terminal states are defined to 0
Vtd = c(0,0.5*rep(1,nStates),0)
reward=0
for (i in nEpisodes){
#perform a random walk
#states are [1=X,2,3,4,5,6,7=X] == [X,A,B,C,D,E,X]
#and we start at C=4 (X is a terminal code)
state=4
R=0
while (1<state && state<7){
if (sample(c(0,1),1)==1){step1=state+1}
else {step1=state-1}
#assign a reward for this step
if (step1==7){reward=0}
else{reward= reward+1}
R=R+reward
# update Vtd
Vtd[state]= Vtd[state] + alpha*(reward + gamma*Vtd[step1] - Vtd[state])
state=step1
cat('state',state,"reward: ",reward,"\n")
}
cat("\n\n")
# return(c('state',state))
# return(c('reward',reward))
}
}
TD0(1000,0.1)
source('~/.active-rstudio-document', echo=TRUE)
TD0(1000,0.1)
TD0(1000,0.1)
TD0(1000,0.1)
TD0(1000,0.1)
source('~/.active-rstudio-document', echo=TRUE)
TD0(100,0.1)
(1:5)/6
plot(1:5,p,type="p")
plot(1:5,p,type="p")
#True proba of rearching the most right case
p=(1:5)/6
plot(1:5,p,type="p")
plot(1:5,p,type="b")
source('~/Data mining and decision making/TD(0).R', echo=TRUE)
knitr::opts_chunk$set(echo = TRUE)
source('FonctionsSeriesChrono.R') ## needs to be in the working directory
source('FonctionsSeriesChrono.R') ## needs to be in the working directory
setwd("~/Time series analysis/TP3")
setwd("~/Time series analysis/TP3")
source('FonctionsSeriesChrono.R') ## needs to be in the working directory
milk <- read.table('milk.txt', header = F)
plot(ald, ylab = "Milk Production (mensual)", main = "months")
plot(milk, ylab = "Milk Production (mensual)", main = "months")
View(milk)
View(milk)
plot(milk, ylab = "Milk Production (mensual)", main = "months")
plot(milk, ylab = "Milk Production (mensual)", main = "months")
View(milk)
View(milk)
debugSource('C:/Users/p/Documents/GitHub/markov-processes/value_iteration.R', echo=TRUE)
\begin{figure}[h]
\caption{Schéma du jeu de l'oie modifié}
\includegraphics[scale=0.2]{jeu de l'oie modifié.PNG}
\centering
\end{figure}
knit_with_parameters('C:/Users/p/Documents/GitHub/markov-processes/LSINF2275_GUERIN_TRUFFAUT.Rmd')
v_iter(p_secure1_prison,p_normal1_prison,p_risk1_prison,Cost_prison)
source('C:/Users/p/Documents/GitHub/markov-processes/Result_Analysis_prison.R', echo=TRUE)
source('C:/Users/p/Documents/GitHub/markov-processes/Result_Analysis_prison.R', echo=TRUE)
source('C:/Users/p/Documents/GitHub/markov-processes/Result_Analysis_prison.R', echo=TRUE)
source('C:/Users/p/Documents/GitHub/markov-processes/Result_Analysis_prison.R', echo=TRUE)
v_iter(p_secure1_prison,p_normal1_prison,p_risk1_prison,Cost_prison)
v_iter(p_secure2_prison,p_normal2_prison,p_risk2_prison,Cost_prison)[[1]]
v_iter(p_secure1_prison,p_normal1_prison,p_risk1_prison,Cost_prison)[[1]]
matrix(4,16)
source('C:/Users/p/Documents/GitHub/markov-processes/Result_Analysis.R', echo=TRUE)
source('C:/Users/p/Documents/GitHub/markov-processes/Result_Analysis.R', echo=TRUE)
v_iter(p_secure1,p_normal1,p_risk1,Cost)[[1]]
v_iter(p_secure2,p_normal2,p_risk2,Cost)[[1]]
matrix(c(3,3,2,2,3,3,3,3,3,3,3,3,1,3,'NA'),
c(3,3,2,2,3,3,3,3,2,1,3,3,2,1,'NA'),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1),
nrow=4,
ncol=16)
matrix(c(3,3,2,2,3,3,3,3,3,3,3,3,1,3,'NA'),
c(3,3,2,2,3,3,3,3,2,1,3,3,2,1,'NA'),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1),
nrow=4,
ncol=15)
matrix(c(c(3,3,2,2,3,3,3,3,3,3,3,3,1,3,'NA'),
c(3,3,2,2,3,3,3,3,2,1,3,3,2,1,'NA'),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15)
matrix(data=c(c(3,3,2,2,3,3,3,3,3,3,3,3,1,3,'NA'),
c(3,3,2,2,3,3,3,3,2,1,3,3,2,1,'NA'),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15)
c(c(3,3,2,2,3,3,3,3,3,3,3,3,1,3,'NA'),
c(3,3,2,2,3,3,3,3,2,1,3,3,2,1,'NA'),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1))
matrix(data=c(c(3,3,2,2,3,3,3,3,3,3,3,3,1,3,'NA'),
c(3,3,2,2,3,3,3,3,2,1,3,3,2,1,'NA'),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
matrix(data=c(c(3,3,2,2,3,3,3,3,3,'NA'3,3,3,1,3,),
c(3,3,2,2,3,3,3,3,2,,'NA'1,3,3,2,1,),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
matrix(data=c(c(3,3,2,2,3,3,3,3,3,'NA',3,3,3,1,3),
c(3,3,2,2,3,3,3,3,2,'NA',1,3,3,2,1),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
policy_matrix <- matrix(data=c(c(3,3,2,2,3,3,3,3,3,'NA',3,3,3,1,3),
c(3,3,2,2,3,3,3,3,2,'NA',1,3,3,2,1),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
rownames(policy_matrix)<- c("Jeu avec la règle 1","jeu avec la règle 2","jeu avec la règle 1,avec ralentisseur","jeu avec la règle 2,avec ralentisseur")
kable(round(result_matrix_rule2_prison,2),caption="dé choisi par la police optimale en fonction de la case")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
# Chunk 2
#---------CHARGEMENT DES SCRIPTS---------------------------------------------
#setwd("~/GitHub/markov-processes")
setwd("C:/Users/p/Documents/GitHub/markov-processes")
source('transition_matrix.R')
source('value_iteration.R')
source('simulation_game.R')
library(knitr)
# Chunk 3
result_matrix_rule1 <- read.table("result_matrix_rule1.txt")
colnames(result_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule1)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14")
kable(round(result_matrix_rule1,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 4
variance_matrix_rule1<-read.table("variance_matrix_rule1.txt")
colnames(variance_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule1,2),caption="Variance des différentes stratégies")
# Chunk 5
result_matrix_rule2 <- read.table("result_matrix_rule2.txt")
colnames(result_matrix_rule2) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule2)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14")
kable(round(result_matrix_rule2,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 6
variance_matrix_rule2<-read.table("variance_matrix_rule2.txt")
colnames(variance_matrix_rule2) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule2,2),caption="Variance des différentes stratégies")
# Chunk 7
result_matrix_rule1_prison <- read.table("result_matrix_rule1_prison.txt")
colnames(result_matrix_rule1_prison) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule1_prison)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14","Square 15")
kable(round(result_matrix_rule1_prison,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 8
result_matrix_rule2_prison <- read.table("result_matrix_rule2_prison.txt")
colnames(result_matrix_rule2_prison) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule2_prison)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14","Square 15")
kable(round(result_matrix_rule2_prison,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 9
policy_matrix <- matrix(data=c(c(3,3,2,2,3,3,3,3,3,'NA',3,3,3,1,3),
c(3,3,2,2,3,3,3,3,2,'NA',1,3,3,2,1),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
rownames(policy_matrix)<- c("Jeu avec la règle 1","jeu avec la règle 2","jeu avec la règle 1,avec ralentisseur","jeu avec la règle 2,avec ralentisseur")
kable(round(result_matrix_rule2_prison,2),caption="dé choisi par la police optimale en fonction de la case")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
# Chunk 2
#---------CHARGEMENT DES SCRIPTS---------------------------------------------
#setwd("~/GitHub/markov-processes")
setwd("C:/Users/p/Documents/GitHub/markov-processes")
source('transition_matrix.R')
source('value_iteration.R')
source('simulation_game.R')
library(knitr)
# Chunk 3
result_matrix_rule1 <- read.table("result_matrix_rule1.txt")
colnames(result_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule1)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14")
kable(round(result_matrix_rule1,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 4
variance_matrix_rule1<-read.table("variance_matrix_rule1.txt")
colnames(variance_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule1,2),caption="Variance des différentes stratégies")
# Chunk 5
result_matrix_rule2 <- read.table("result_matrix_rule2.txt")
colnames(result_matrix_rule2) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule2)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14")
kable(round(result_matrix_rule2,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 6
variance_matrix_rule2<-read.table("variance_matrix_rule2.txt")
colnames(variance_matrix_rule2) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule2,2),caption="Variance des différentes stratégies")
# Chunk 7
result_matrix_rule1_prison <- read.table("result_matrix_rule1_prison.txt")
colnames(result_matrix_rule1_prison) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule1_prison)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14","Square 15")
kable(round(result_matrix_rule1_prison,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 8
result_matrix_rule2_prison <- read.table("result_matrix_rule2_prison.txt")
colnames(result_matrix_rule2_prison) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule2_prison)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14","Square 15")
kable(round(result_matrix_rule2_prison,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 9
policy_matrix <- matrix(data=c(c(3,3,2,2,3,3,3,3,3,'NA',3,3,3,1,3),
c(3,3,2,2,3,3,3,3,2,'NA',1,3,3,2,1),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
rownames(policy_matrix)<- c("Jeu avec la règle 1","jeu avec la règle 2","jeu avec la règle 1,avec ralentisseur","jeu avec la règle 2,avec ralentisseur")
kable(round(result_matrix_rule2_prison,2),caption="dé choisi par la police optimale en fonction de la case")
# Chunk 1: setup
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(comment = "")
# Chunk 2
#---------CHARGEMENT DES SCRIPTS---------------------------------------------
#setwd("~/GitHub/markov-processes")
setwd("C:/Users/p/Documents/GitHub/markov-processes")
source('transition_matrix.R')
source('value_iteration.R')
source('simulation_game.R')
library(knitr)
# Chunk 3
result_matrix_rule1 <- read.table("result_matrix_rule1.txt")
colnames(result_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule1)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14")
kable(round(result_matrix_rule1,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 4
variance_matrix_rule1<-read.table("variance_matrix_rule1.txt")
colnames(variance_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule1,2),caption="Variance des différentes stratégies")
# Chunk 5
result_matrix_rule2 <- read.table("result_matrix_rule2.txt")
colnames(result_matrix_rule2) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule2)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14")
kable(round(result_matrix_rule2,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 6
variance_matrix_rule2<-read.table("variance_matrix_rule2.txt")
colnames(variance_matrix_rule2) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule2,2),caption="Variance des différentes stratégies")
# Chunk 7
result_matrix_rule1_prison <- read.table("result_matrix_rule1_prison.txt")
colnames(result_matrix_rule1_prison) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule1_prison)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14","Square 15")
kable(round(result_matrix_rule1_prison,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 8
result_matrix_rule2_prison <- read.table("result_matrix_rule2_prison.txt")
colnames(result_matrix_rule2_prison) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
rownames(result_matrix_rule2_prison)<- c("Square 1","Square 2","Square 3","Square 4","Square 5","Square 6","Square 7","Square 8","Square 9","Square 10","Square 11","Square 12","Square 13","Square 14","Square 15")
kable(round(result_matrix_rule2_prison,2),caption="Nombre d'itérations moyen en fonction de la case de départ")
# Chunk 9
policy_matrix <- matrix(data=c(c(3,3,2,2,3,3,3,3,3,'NA',3,3,3,1,3),
c(3,3,2,2,3,3,3,3,2,'NA',1,3,3,2,1),
c(3,3,2,2,3,3,3,3,3,3,3,3,3,1,3),
c(3,3,2,2,3,3,3,3,3,2,1,3,3,2,1)),
nrow=4,
ncol=15,
byrow=TRUE)
rownames(policy_matrix)<- c("Jeu avec la règle 1","jeu avec la règle 2","jeu avec la règle 1,avec ralentisseur","jeu avec la règle 2,avec ralentisseur")
kable(round(result_matrix_rule2_prison,2),caption="dé choisi par la police optimale en fonction de la case")
variance_matrix_rule1<-read.table("variance_matrix_rule1.txt")
colnames(variance_matrix_rule1) <- c("Optimal policy","Dice 1","Dice 2", "Dice 3", "Random dice")
kable(round(variance_matrix_rule1,2),caption="Variance des différentes stratégies")
```
